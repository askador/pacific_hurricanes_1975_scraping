My approach to extracting hurricane data from the 1975 Pacific hurricane season Wikipedia page involved a combination of web scraping and natural language processing techniques. I used the Python library BeautifulSoup to scrape the Wikipedia page, extracting the relevant sections for each hurricane. For data extraction, I used several large language models (GPT-3.5 Turbo, GPT-4o, Llama 2) to parse the unstructured text and extract structured information about each hurricane, including its name, start and end dates, number of deaths, and affected areas in different formats: CSV and JSON, as well as using Function Calling. After some testing in Jupyter Notebook, I chose GPT-3.5 Turbo since it was more consistent, and cheaper (Llama 2 is free but takes a lot of generation time).

To assess the quality of the extracted data, I applied various validation checks to ensure completeness and correctness. For instance, I checked that dates were in the correct format (YYYY-MM-DD), verified that the number of deaths was an integer, and ensured that affected areas were extracted as a list of strings. To further ensure data integrity, I manually cross-referenced the extracted data with information available on the Wikipedia page, paying attention to potential discrepancies in text parsing. Additionally, I implemented code to identify any missing or malformed data that may require manual correction or rescraping.